import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AcademicScraper:
    def __init__(self):
        self.results = {"academic": [], "policy": [], "update_time": ""}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Referer': 'https://www.baidu.com/'
        }

    def fetch(self, url, site_name, category, selector=None):
        try:
            # é’ˆå¯¹å›½å†…éƒ¨åˆ†é«˜æ ¡/æ”¿åºœç½‘ï¼Œå°è¯•å¢åŠ ç‰¹å®šè¯·æ±‚å¤´ç»•è¿‡æµ·å¤–æ‹¦æˆª
            res = requests.get(url, headers=self.headers, timeout=25, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'lxml')
            
            target = soup.select_one(selector) if selector else soup
            if not target: target = soup

            links = target.find_all('a')
            count = 0
            
            # ğŸ”´ æ ¸å¿ƒæ”¹è¿›ï¼šæå…¶ä¸¥æ ¼çš„é»‘åå•ï¼Œå½»åº•è¿‡æ»¤å¤‡æ¡ˆå·å’Œæ— æ•ˆé“¾æ¥
            blacklist = ['å¤‡æ¡ˆ', 'ç‰ˆæƒ', 'ICP', 'å…¬ç½‘å®‰å¤‡', 'ç™»å½•', 'æ³¨å†Œ', 'About', 'English', 'FranÃ§ais', 'æ›´å¤š', 'è”ç³»', 'è¿”å›', 'é¦–é¡µ', 'å°æœ¨è™«è®ºå›']
            
            for link in links:
                title = link.get_text().strip()
                href = link.get('href', '')
                full_url = urljoin(url, href)
                
                # ğŸŸ¡ æ ¸å¿ƒæ”¹è¿›ï¼šæ ‡é¢˜å¿…é¡»åŒ…å«æ–°é—»ç‰¹å¾ï¼Œä¸”é•¿åº¦é€‚ä¸­
                if 12 <= len(title) <= 60 and full_url.startswith('http'):
                    if not any(word in title for word in blacklist):
                        # é¢å¤–æ ¡éªŒï¼šæ’é™¤é‚£äº›çº¯æ•°å­—æˆ–æ˜æ˜¾ä¸æ˜¯æ–°é—»çš„é“¾æ¥
                        if title.isdigit() or len(set(title)) < 5: continue
                        
                        self.results[category].append({
                            "title": f"[{site_name}] {title}",
                            "url": full_url
                        })
                        count += 1
                if count >= 10: break
            
            print(f"âœ… {site_name} æŠ“å–æˆåŠŸ: {count} æ¡")
        except Exception as e:
            print(f"âŒ {site_name} å¤±è´¥: {e}")

    def run(self):
        tasks = [
            # 1. ç§‘å­¦ç½‘ (å·²ç»é€šäº†ï¼Œä¿æŒåŸæ ·)
            {"site": "ç§‘å­¦ç½‘", "url": "https://news.sciencenet.cn/", "cate": "academic", "sel": "#list_inner"},
            
            # 2. ç¤¾ç§‘ç½‘ (å°è¯•æ›´æ¢ä¸ºæ–°é—»å­é¢‘é“ï¼Œç»•è¿‡é¦–é¡µæ‹¦æˆª)
            {"site": "ç¤¾ç§‘ç½‘", "url": "http://www.cssn.cn/zx/zx_gx/", "cate": "academic", "sel": ".list_ul"},
            
            # 3. å­¦æœ¯ä¼šè®® (æ›´æ¢å…·ä½“åˆ†ç±»é¡µ)
            {"site": "å­¦æœ¯ä¼šè®®", "url": "https://www.meeting.edu.cn/zh/meeting/list", "cate": "academic", "sel": ".list-item-box"},
            
            # 4. å°æœ¨è™« (é”å®šä»Šæ—¥å¤´æ¡åŒº)
            {"site": "å°æœ¨è™«", "url": "http://muchong.com/bbs/index.php?gid=29", "cate": "policy", "sel": ".stitle"},
            
            # 5. æ–‡çŒ®ä¸­å¿ƒ (æ”¿åºœèƒŒæ™¯ï¼Œæ˜“è¢«æŒ¡)
            {"site": "æ–‡çŒ®ä¸­å¿ƒ", "url": "http://www.ncpssd.org/notice.aspx", "cate": "policy", "sel": ".list_con"}
        ]

        for t in tasks:
            self.fetch(t["url"], t["site"], t["cate"], t["sel"])
            time.sleep(2)

        self.results["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open("news.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    AcademicScraper().run()
