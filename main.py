import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from urllib.parse import urljoin
import urllib3

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AcademicScraper:
    def __init__(self):
        self.results = {"academic": [], "policy": [], "update_time": ""}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Referer': 'https://www.baidu.com/'
        }

    def fetch(self, url, site_name, category, selector=None):
        try:
            res = requests.get(url, headers=self.headers, timeout=30, verify=False)
            res.encoding = res.apparent_encoding
            soup = BeautifulSoup(res.text, 'lxml')
            
            target = soup.select_one(selector) if selector else soup
            if not target: target = soup

            links = target.find_all('a')
            count = 0
            
            # ğŸ›‘ æå…¶ä¸¥æ ¼çš„è¿‡æ»¤é»‘åå•
            blacklist = ['å¤‡æ¡ˆ', 'ç‰ˆæƒ', 'ICP', 'å…¬å®‰', 'ç™»å½•', 'æ³¨å†Œ', 'About', 'English', 'æ›´å¤š', 'é¦–é¡µ', 'è”ç³»', 'äº’åŠ¨å¹³å°', 'è¿”å›', 'è®ºå›']
            # âœ… æ­£å‘ç‰¹å¾ï¼šæ ‡é¢˜ä¸­é€šå¸¸åŒ…å«çš„å­¦æœ¯/æ–°é—»å…³é”®è¯
            keywords = ['é¡¹ç›®', 'è·', 'æ­ç¤º', 'ç ”ç©¶', 'é€šçŸ¥', 'å…¬å‘Š', 'ä¼šè®®', 'å‘å±•', 'å»ºè®¾', 'å‘å¸ƒ', 'æˆæœ', 'åŠå¯¼ä½“', 'è£…ç½®', 'æœºåˆ¶', 'çªç ´']

            for link in links:
                title = link.get_text().strip()
                href = link.get('href', '')
                full_url = urljoin(url, href)
                
                # è¿‡æ»¤é€»è¾‘ï¼š1. é•¿åº¦å¿…é¡»åœ¨ 12-60 ä¹‹é—´ï¼› 2. ä¸å«é»‘åå•è¯æ±‡ï¼› 3. ä¸èƒ½æ˜¯çº¯æ•°å­—
                if 12 <= len(title) <= 60 and full_url.startswith('http'):
                    if not any(word in title for word in blacklist):
                        # æ’é™¤æ‰ç±»ä¼¼ "äº¬å…¬ç½‘å®‰å¤‡xxx" æˆ–è€… "å°æœ¨è™«-å­¦æœ¯..." è¿™ç§å›ºå®šæ ‡é¢˜
                        if "1101" in title or "å¤‡" in title: continue
                        
                        self.results[category].append({
                            "title": f"[{site_name}] {title}",
                            "url": full_url
                        })
                        count += 1
                if count >= 8: break
            print(f"âœ… {site_name} æœ‰æ•ˆæ•°æ®: {count} æ¡")
        except Exception as e:
            print(f"âŒ {site_name} å¤±è´¥: {e}")

    def run(self):
        tasks = [
            # å­¦æœ¯å‰æ²¿ (Academic)
            {"site": "ç§‘å­¦ç½‘", "url": "https://news.sciencenet.cn/", "cate": "academic", "sel": "#list_inner"},
            # è°ƒæ•´ç¤¾ç§‘ç½‘é“¾æ¥ï¼Œç›´æ¥è¿›å…¥â€œé«˜å±‚åŠ¨æ€â€å­æ ç›®
            {"site": "ç¤¾ç§‘å‰æ²¿", "url": "http://www.cssn.cn/zx/zx_gx/", "cate": "academic", "sel": ".list_ul"},
            
            # æ”¿ç­–/ä¼šè®® (Policy)
            # è°ƒæ•´ä¼šè®®åœ¨çº¿é“¾æ¥ï¼Œé”å®šæœ€æ–°å‘å¸ƒ
            {"site": "å­¦æœ¯ä¼šè®®", "url": "https://www.meeting.edu.cn/zh/meeting/list", "cate": "policy", "sel": ".list-item-box"},
            # é”å®šå°æœ¨è™«çš„â€œå­¦æœ¯åŠ¨æ€â€å…·ä½“ç‰ˆå—
            {"site": "å°æœ¨è™«", "url": "http://muchong.com/bbs/forumdisplay.php?fid=330", "cate": "policy", "sel": ".stitle"}
        ]

        for t in tasks:
            self.fetch(t["url"], t["site"], t["cate"], t["sel"])
            time.sleep(2)

        self.results["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with open("news.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)

if __name__ == "__main__":
    AcademicScraper().run()
