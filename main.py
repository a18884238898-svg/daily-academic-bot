import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from urllib.parse import urljoin

# æ›´å¼ºå¤§çš„æµè§ˆå™¨ä¼ªè£…
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,en;q=0.5,en-US;q=0.3',
}

def smart_fetch(url, site_name):
    items = []
    try:
        # verify=False ç»•è¿‡è¯ä¹¦é”™è¯¯ï¼Œtimeout å¢åŠ åˆ° 20 ç§’
        response = requests.get(url, headers=HEADERS, timeout=20, verify=False)
        response.encoding = response.apparent_encoding # è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼ˆè§£å†³ä¹±ç ï¼‰
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ ¸å¿ƒé€»è¾‘ï¼šä¸å†æ­»ç£•æŸä¸ª IDï¼Œè€Œæ˜¯æŠ“å–å‰ 15 ä¸ªåŒ…å«æ ‡é¢˜æ–‡å­—çš„é“¾æ¥
        links = soup.find_all('a')
        count = 0
        for link in links:
            title = link.get_text().strip()
            href = link.get('href', '')
            
            # è¿‡æ»¤é€»è¾‘ï¼šæ ‡é¢˜é•¿åº¦åœ¨ 8-40 ä¹‹é—´ï¼Œä¸”ä¸æ˜¯â€œæ›´å¤šâ€ã€â€œé¦–é¡µâ€ç­‰å¹²æ‰°è¯
            if 8 <= len(title) <= 40 and href and not href.startswith('javascript'):
                full_url = urljoin(url, href)
                if full_url.startswith('http'):
                    items.append({"title": f"[{site_name}] {title}", "url": full_url})
                    count += 1
            if count >= 8: break # æ¯ä¸ªç«™åªå–å‰ 8 æ¡ï¼Œé˜²æ­¢ App åˆ·ä¸åŠ¨
            
        print(f"âœ… {site_name} æŠ“å–æˆåŠŸ: {len(items)} æ¡")
    except Exception as e:
        print(f"âŒ {site_name} æŠ“å–å¤±è´¥: {e}")
    return items

def main():
    # é‡æ–°æ¢³ç†çš„æœ€ç¨³å¥å…¥å£åœ°å€
    tasks = [
        # å­¦æœ¯å‰æ²¿
        {"cate": "academic", "site": "ç§‘å­¦ç½‘", "url": "https://news.sciencenet.cn/sublist.aspx?type=1&id=1"},
        {"cate": "academic", "site": "ç¤¾ç§‘ç½‘", "url": "http://www.cssn.cn/zx/zx_gx/"},
        {"cate": "academic", "site": "PubScholar", "url": "https://pubscholar.cn/news/index"},
        
        # æ”¿ç­–/ä¼šè®®
        {"cate": "policy", "site": "å­¦æœ¯ä¼šè®®", "url": "https://www.meeting.edu.cn/zh/meeting/list"},
        {"cate": "policy", "site": "å­¦ä½ä¸­å¿ƒ", "url": "https://www.cdgdc.edu.cn/xwyyjsjyxx/index.shtml"},
        {"cate": "policy", "site": "ç¤¾ç§‘æ–‡çŒ®", "url": "http://www.ncpssd.org/notice.aspx"}
    ]

    news_data = {"academic": [], "policy": [], "update_time": ""}

    for t in tasks:
        results = smart_fetch(t['url'], t['site'])
        news_data[t['cate']].extend(results)
        time.sleep(2) # å¢åŠ å»¶è¿Ÿï¼Œé˜²æ­¢è¢«å°

    news_data["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    with open("news.json", "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=4)
    print("ğŸ‰ æŠ“å–ä»»åŠ¡å®Œæˆï¼")

if __name__ == "__main__":
    main()
