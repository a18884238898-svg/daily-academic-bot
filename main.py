import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from urllib.parse import urljoin
import urllib3

# ç¦ç”¨ SSL è¯ä¹¦è­¦å‘Šï¼ˆé˜²æ­¢éƒ¨åˆ†æ”¿åºœç½‘ç«™è¯ä¹¦è¿‡æœŸå¯¼è‡´ç¨‹åºä¸­æ–­ï¼‰
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class AcademicScraper:
    def __init__(self):
        self.results = {"academic": [], "policy": [], "update_time": ""}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }

    def fetch(self, url, site_name, category, selector=None):
        print(f"ğŸ“¡ æ­£åœ¨å°è¯•æŠ“å–: {site_name}...")
        try:
            # ä¼ªè£…æ¥æºï¼šè®©æœåŠ¡å™¨è®¤ä¸ºæˆ‘ä»¬æ˜¯ä»ç™¾åº¦æˆ–è€…çŸ¥ç½‘è¿‡æ¥çš„
            current_headers = self.headers.copy()
            current_headers['Referer'] = 'https://www.baidu.com/'
            
            response = requests.get(url, headers=current_headers, timeout=25, verify=False)
            response.encoding = response.apparent_encoding # è‡ªåŠ¨çº æ­£ GBK/UTF-8 ç¼–ç 
            
            if response.status_code != 200:
                print(f"âš ï¸ {site_name} è¿”å›çŠ¶æ€ç : {response.status_code} (å¯èƒ½è¢«å±è”½)")
                return

            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾å†…å®¹ï¼šå¦‚æœæ²¡è®¾é€‰æ‹©å™¨ï¼Œåˆ™å…¨ç«™æœå¯» <a> æ ‡ç­¾
            target_area = soup.select_one(selector) if selector else soup
            if not target_area:
                target_area = soup

            links = target_area.find_all('a')
            count = 0
            
            # è¿‡æ»¤é€»è¾‘ï¼šå»æ‰çŸ­è¯ï¼ˆå¦‚â€œæ›´å¤šâ€ã€â€œç™»å½•â€ï¼‰ï¼Œä¿ç•™é•¿æ ‡é¢˜
            blacklist = ['å¤‡æ¡ˆ', 'ç‰ˆæƒ', 'ç™»å½•', 'æ³¨å†Œ', 'About', 'English', 'FranÃ§ais', 'æ›´å¤š', 'è”ç³»']
            
            for link in links:
                title = link.get_text().strip()
                href = link.get('href', '')
                
                # è¡¥å…¨ URL
                full_url = urljoin(url, href)
                
                # åˆ¤å®šä¸ºæœ‰æ•ˆæ–°é—»çš„æ¡ä»¶ï¼šæ ‡é¢˜é•¿åº¦åœ¨ 12-50 ä¹‹é—´ï¼Œä¸”ä¸åœ¨é»‘åå•ä¸­
                if 12 <= len(title) <= 55 and full_url.startswith('http'):
                    if not any(word in title for word in blacklist):
                        self.results[category].append({
                            "title": f"[{site_name}] {title}",
                            "url": full_url
                        })
                        count += 1
                
                if count >= 8: break # æ¯ä¸ªç«™ç‚¹æœ€å¤šå– 8 æ¡
            
            print(f"âœ… {site_name} æˆåŠŸè·å– {count} æ¡")
            
        except Exception as e:
            print(f"âŒ {site_name} æŠ“å–å¼‚å¸¸: {str(e)}")

    def run(self):
        # --- ä»»åŠ¡é…ç½®æ¸…å• ---
        tasks = [
            # å­¦æœ¯å‰æ²¿ (Academic)
            {"site": "ç§‘å­¦ç½‘", "url": "https://news.sciencenet.cn/", "cate": "academic", "sel": "#list_inner"},
            {"site": "ç¤¾ç§‘ç½‘", "url": "http://www.cssn.cn/zx/zx_gx/", "cate": "academic", "sel": ".list_ul"},
            {"site": "PubScholar", "url": "https://pubscholar.cn/news/index", "cate": "academic", "sel": ".list-content"},
            {"site": "å­¦æœ¯ä¼šè®®", "url": "https://www.meeting.edu.cn/zh/meeting/list", "cate": "academic", "sel": ".list-item-box"},
            
            # æ”¿ç­–è®ºå› (Policy)
            {"site": "å­¦ä½ä¸­å¿ƒ", "url": "https://www.cdgdc.edu.cn/xwyyjsjyxx/index.shtml", "cate": "policy", "sel": ".news_list"},
            {"site": "æ–‡çŒ®ä¸­å¿ƒ", "url": "http://www.ncpssd.org/notice.aspx", "cate": "policy", "sel": ".list_con"},
            {"site": "å°æœ¨è™«", "url": "http://muchong.com/bbs/index.php?gid=29", "cate": "policy", "sel": ".stitle"}
        ]

        for t in tasks:
            self.fetch(t["url"], t["site"], t["cate"], t["sel"])
            time.sleep(2) # ç¤¼è²Œå»¶è¿Ÿï¼Œé˜²æ­¢ GitHub IP è¢«å°

        # æ›´æ–°æ—¶é—´æˆ³
        self.results["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # å†™å…¥æ–‡ä»¶
        with open("news.json", "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=4)
        print("ğŸ‰ æŠ“å–ä»»åŠ¡åœ†æ»¡ç»“æŸï¼")

if __name__ == "__main__":
    scraper = AcademicScraper()
    scraper.run()
