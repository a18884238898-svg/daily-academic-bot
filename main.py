import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import time
from urllib.parse import urljoin

# æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨å¤´éƒ¨
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}

def safe_fetch(url, selector, site_name, encoding='utf-8'):
    """
    å¸¦å®¹é”™çš„æŠ“å–å‡½æ•°ï¼šå¦‚æœä¸€ä¸ªç½‘ç«™æŒ‚äº†ï¼Œè¿”å›ç©ºåˆ—è¡¨è€Œä¸æ˜¯å´©æºƒ
    """
    items = []
    try:
        # verify=False è§£å†³éƒ¨åˆ†æ”¿åºœç½‘ç«™è¯ä¹¦è¿‡æœŸçš„æŠ¥é”™
        response = requests.get(url, headers=HEADERS, timeout=15, verify=False)
        response.encoding = encoding
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å¯»æ‰¾åŒ¹é…çš„å®¹å™¨
        container = soup.select_one(selector)
        if container:
            # é™åˆ¶æ¯ä¸ªæºæŠ“å– 5 æ¡
            links = container.find_all('a', limit=8)
            for link in links:
                title = link.get_text().strip()
                href = link.get('href', '')
                
                if not href: continue
                # è¡¥å…¨ç›¸å¯¹è·¯å¾„ (ä¾‹å¦‚ /news/123.html -> http://site.com/news/123.html)
                full_url = urljoin(url, href)
                
                # è¿‡æ»¤æ‰å¹²æ‰°é¡¹ï¼ˆå¦‚â€œæ›´å¤šâ€ã€â€œæŸ¥çœ‹è¯¦ç»†â€ç­‰çŸ­è¯­ï¼‰
                if len(title) > 6 and full_url.startswith('http'):
                    items.append({"title": f"[{site_name}] {title}", "url": full_url})
        print(f"âœ… {site_name} æŠ“å–æˆåŠŸ: {len(items)} æ¡")
    except Exception as e:
        print(f"âŒ {site_name} æŠ“å–å¤±è´¥: {e}")
    return items

def main():
    # ä»»åŠ¡é…ç½®æ¸…å•
    tasks = [
        # --- å­¦æœ¯å‰æ²¿ ---
        {"cate": "academic", "site": "ç§‘å­¦ç½‘", "url": "https://news.sciencenet.cn/", "selector": "#list_inner"},
        {"cate": "academic", "site": "ç¤¾ç§‘ç½‘", "url": "http://www.cssn.cn/zx/zx_gx/", "selector": ".list_ul"},
        {"cate": "academic", "site": "PubScholar", "url": "https://pubscholar.cn/news/index", "selector": ".list-content"},
        
        # --- æ”¿ç­–/ä¼šè®® ---
        {"cate": "policy", "site": "å­¦æœ¯ä¼šè®®", "url": "https://www.meeting.edu.cn/zh/meeting/list", "selector": ".list-item-box"},
        {"cate": "policy", "site": "å­¦ä½ä¸­å¿ƒ", "url": "https://www.cdgdc.edu.cn/xwyyjsjyxx/index.shtml", "selector": ".news_list"},
        {"cate": "policy", "site": "ç¤¾ç§‘æ–‡çŒ®ä¸­å¿ƒ", "url": "http://www.ncpssd.org/notice.aspx", "selector": ".list_con"}
    ]

    news_data = {"academic": [], "policy": [], "update_time": ""}

    for t in tasks:
        results = safe_fetch(t['url'], t['selector'], t['site'])
        news_data[t['cate']].extend(results)
        time.sleep(1) # é—´éš”1ç§’ï¼Œé¿å…è¢«å°IP

    # è®°å½•æœ€åæ›´æ–°æ—¶é—´
    news_data["update_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # å†™å…¥ JSON
    with open("news.json", "w", encoding="utf-8") as f:
        json.dump(news_data, f, ensure_ascii=False, indent=4)
    print("ğŸ‰ ä»»åŠ¡åœ†æ»¡å®Œæˆï¼")

if __name__ == "__main__":
    main()
